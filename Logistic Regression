import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
X,y = make_classification(n_samples=35040,n_classes=2,n_features=3,n_informative=2,n_redundant=1,
                           weights=[0.999,0.001],class_sep=1.0)
df=pd.DataFrame(data=X,columns=['Temp','Humidity','Crime'])
df['y']=y
df['Temp']=df['Temp']-min(df['Temp'])
maxt=max(df['Temp'])
df['Temp']=90*df['Temp']/maxt
df['Humidity']=df['Humidity']-min(df['Humidity'])
maxh=max(df['Humidity'])
df['Humidity']=100*df['Humidity']/maxh
df['Crime']=df['Crime']-min(df['Crime'])
maxc=max(df['Crime'])
df['Crime']=10*df['Crime']/maxc
df.hist('Temp')
df.hist('Humidity')
df.hist('Crime')
sum(df['y']==1)
df.head(10)
df.describe()

###Logistic Regression undersampling
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegressionCV
from sklearn.metrics import classification_report
df0=df[df['y']==0].sample(800)
df1=df[df['y']==1]
df_balanced = pd.concat([df0,df1],axis=0)
df_balanced.describe()
df_balanced.hist('y')
plt.title("Relative frequency of positive and negative classes\n in the balanced (under-sampled) dataset")
log_model_balanced = LogisticRegressionCV(cv=5,class_weight='balanced')
X_train, X_test, y_train, y_test = train_test_split(df_balanced.drop('y',axis=1), 
                                                    df_balanced['y'], test_size=0.30)
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
X_train = scaler.fit_transform(X_train)
log_model_balanced.fit(X_train,y_train)
print(classification_report(y_test,log_model_balanced.predict(X_test)))

#####
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
n_neg = [i for i in range(200,4200,200)]

df1=df[df['y']==1]
F1_scores=[]
precision_scores=[]
recall_scores=[]

for num in n_neg:
    # Create under-sampled data sets
    df0=df[df['y']==0].sample(num)
    df_balanced = pd.concat([df0,df1],axis=0)
    # Create model with 'class_weight=balanced' and 5-fold cross-validation
    log_models=LogisticRegressionCV(cv=5,class_weight='balanced')
    # Create test/train splits
    X_train, X_test, y_train, y_test = train_test_split(df_balanced.drop('y',axis=1), 
                                                    df_balanced['y'], test_size=0.30)
    # Min-max scale the training data
    X_train = scaler.fit_transform(X_train)
    
    # Fit the logistic regression model
    log_models.fit(X_train,y_train)
    
    # Calculate various scores
    F1_scores.append(f1_score(y_test,log_models.predict(X_test)))
    precision_scores.append(precision_score(y_test,log_models.predict(X_test)))
    recall_scores.append(recall_score(y_test,log_models.predict(X_test)))
plt.scatter(n_neg,F1_scores,color='green',edgecolor='black',alpha=0.6,s=100)
plt.title("F1-score as function of negative samples")
plt.grid(True)
plt.ylabel("F1-score")
plt.xlabel("Number of negative samples")
plt.scatter(n_neg,precision_scores,color='orange',edgecolor='black',alpha=0.6,s=100)
plt.title("Precision score as function of negative samples")
plt.grid(True)
plt.ylabel("Precision score")
plt.xlabel("Number of negative samples")
plt.scatter(n_neg,recall_scores,color='blue',edgecolor='black',alpha=0.6,s=100)
plt.title("Recall score as function of negative samples")
plt.grid(True)
plt.ylabel("Recall score")
plt.xlabel("Number of negative samples")